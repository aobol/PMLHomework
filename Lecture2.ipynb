{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "343cac8e-867b-4dc9-97e9-d7e726743e56",
   "metadata": {},
   "source": [
    "# Lecture II: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc37cab0-76ba-49d4-8ce6-7e9340f53f8b",
   "metadata": {},
   "source": [
    "Before we start, I'd like to recommend two very useful pip packages: `tqdm` and `torchsnooper`:\n",
    "- `tqdm` provide a easy wrapper interface for serial operation such as `for` loop or `while` loop. It display the progress/ETA of the serial operation.\n",
    "- `torchsnooper` allows you to monitor the code structure and tensor shape within PyTorch code, I find this extremely useful for debugging the network structure.\n",
    "\n",
    "Let's install these two softwares first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceacef0-29dc-496d-ae7d-40970956047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm --user\n",
    "!pip install torchsnooper --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd32a0-0e28-45a1-bd12-5310275492e0",
   "metadata": {},
   "source": [
    "As usual, let's import some useful module first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed50de-94bd-41ca-a63b-f16a31c9adf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=E1101,R,C\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import gzip\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,RobustScaler,MinMaxScaler\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torchsnooper\n",
    "from torch.cuda.amp import autocast \n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f45ef5-1682-416f-b8e1-cec630c99c61",
   "metadata": {},
   "source": [
    "In this homework, we will train a CNN classifier to classify hand written digits in MNIST dataset.\n",
    "\n",
    "MNIST stands for Modified National Institute of Standards and Technology database, it contains 70,000 hand written digits from 0 - 9, 60,000 of them are training samples and 10,000 of them are testing samples. MNIST dataset is collected by Prof. Yann LeCun and is oftentime called \"the fruitfly of machine learning\". You will see it appear as a preliminary test dataset in all kind of Computer Vision papers. The detail of the dataset can be found at: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Let's first download MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ccb86-59fa-4c63-b09b-01fe683848d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "!tar -zxvf MNIST.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e92aae7-6139-4a26-a6db-42d90fc90591",
   "metadata": {},
   "source": [
    "## Part I: Dataset\n",
    "Before we start building the neural network, it's important to define our dataset object. PyTorch have a neatly organized `Dataset` class that we can use. The class contains several methods:\n",
    "\n",
    "- `__init__`: Containing the initialization information of the dataset, will be called at the beginning of object initialization.\n",
    "\n",
    "- `__len__`: The size of the dataset, will be called when we do `len(dataset_object)`\n",
    "\n",
    "- `__getitem__`: Take in the argument idx and spit out one event per call.\n",
    "\n",
    "Those 3 are mandatory methods that you have to overwrite to create your dataset. You can self-define other methods if you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4880b053-b30c-4d80-9726-bde788712861",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "\n",
    "    def __init__(self,plot=True):\n",
    "        # Load the training and testing dataset\n",
    "        training_images, training_labels = torch.load(os.path.join(os.getcwd(), \"MNIST/processed/training.pt\"))\n",
    "        test_images, test_labels = torch.load(os.path.join(os.getcwd(), \"MNIST/processed/test.pt\"))\n",
    "        # Record down the split point between train and test dataset\n",
    "        self.train_test_split = len(training_labels)\n",
    "        # Record down the overall dataset size\n",
    "        self.size = len(training_labels) + len(test_labels)\n",
    "        \n",
    "        # Concatenate training and testing dataset\n",
    "        self.images = torch.cat([training_images, test_images],dim=0)\n",
    "        self.labels = torch.cat([training_labels, test_labels],dim=0)\n",
    "        \n",
    "        if plot:\n",
    "            # Plot 9 random entries of the dataset\n",
    "            self.plot_data()\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        This function returns the size of overall dataset\n",
    "        '''\n",
    "        return self.size\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        This function extract a single entry from the dataset at the given index idx\n",
    "        '''\n",
    "        return self.images[idx], self.labels[idx]\n",
    "    \n",
    "    def get_train_test_split(self):\n",
    "        '''\n",
    "        This function get the train test split size of the dataset\n",
    "        '''\n",
    "        return self.train_test_split\n",
    "    \n",
    "    def plot_data(self):\n",
    "        '''\n",
    "        This function plots the images and labels of the MNIST dataset\n",
    "        '''\n",
    "        plt.figure(figsize=(12,12))\n",
    "        sample_index = np.random.randint(low=0,high=self.__len__(), size = 9)\n",
    "        for i in range(9):\n",
    "            plt.subplot(3,3, i+1)\n",
    "            image, label = self.__getitem__(sample_index[i])\n",
    "            image = image.data.numpy()\n",
    "            label = label.data.numpy()\n",
    "            plt.imshow(image)\n",
    "            plt.title(\"Handwritten Digit %d\"%(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589019ab-422e-4cb3-8bd9-c262f1632988",
   "metadata": {},
   "source": [
    "We can look at random entries of the MNIST dataset by initializing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f332a1c-e521-435a-9ad6-7a190d3235d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNISTDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce29edd-afc1-4288-af77-4358f432e2bf",
   "metadata": {},
   "source": [
    "## Part II: Convolutional Neural Network\n",
    "In this part, we will get some hands-on experience in building a convolutional neural network.\n",
    "\n",
    "Similar to the PyTorch `Dataset` classs, there is an `nn.Module` class in PyTorch for building arbitrary types of neural networks. The `nn.Module` class contains two methods that have to be overwritten by the user:\n",
    "\n",
    "- `__init__(self)`: initialization of the neural network class\n",
    "\n",
    "- `forward(self,x)`: the forward operation of neural network, take in the training/testing input x and output the network output y\n",
    "\n",
    "Another useful utilities is the `nn.Sequential` method. It allows us to simply stack layers in an python list to form a deep neural network. In the following block, we will build a 2D Convolutional Neural Network using:\n",
    "\n",
    "- `nn.Conv2d`: https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html#conv2d\n",
    "\n",
    "- `nn.BatchNorm2d`: https://pytorch.org/docs/master/generated/torch.nn.BatchNorm2d.html?highlight=batchnorm2d#torch.nn.BatchNorm2d\n",
    "\n",
    "- `nn.Linear`: https://pytorch.org/docs/master/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear\n",
    "\n",
    "- `nn.LeakyReLU`: https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html?highlight=leakyrelu#torch.nn.LeakyReLU\n",
    "\n",
    "- `nn.MaxPool2d`: https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html?highlight=maxpool2d#torch.nn.MaxPool2d\n",
    "\n",
    "- `nn.Sequential`: https://pytorch.org/docs/master/generated/torch.nn.Sequential.html?highlight=sequential#torch.nn.Sequential\n",
    "\n",
    "Since the size of MNIST image is relatively small(${28 \\times 28}$), you should be careful when using the pooling layers, since each ${2 \\times 2}$ pooling layer will reduce the image size to 25%. If you overuse the pooling layer, then the output feature map will be smaller than ${1 \\times 1}$, which necessarily produce an error, $\\textbf{please do not add any extra pooling layers to the network other than the one I have already defined.}$\n",
    "\n",
    "Your goal is to build a $\\textbf{5 convolutional layer, 5 fully connected layer}$ CNN. For each `nn.Conv2d` layers, you need to specify `in_channel`, `out_channel` and `kernel_size`. I will start the first layer, and let you do the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc48f6-768b-49b0-ba02-422d6f7c48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        '''\n",
    "        Initialize the CNN. As mentioned in the lecture, CNN contains 2 parts:\n",
    "            A convolutional feature extractor\n",
    "            A fully connected classifier\n",
    "        '''\n",
    "        \n",
    "        self.feature_extrator = nn.Sequential(\n",
    "            nn.Conv2d(1,5,3),      # in_channel = 1 (one channel per MNIST image), out_channel = 5, kernel = (3,3)\n",
    "            nn.BatchNorm2d(5),     # BatchNorm2D takes the out_channel of previous Conv2D layers\n",
    "            torch.nn.LeakyReLU(),  # Activation function\n",
    "            nn.MaxPool2d(2),        # Pooling Layer\n",
    "        )\n",
    "        \n",
    "        # Fill in the fully connected classifiers using nn.Linear and nn.LeakyReLU\n",
    "        self.fc_classifier = nn.Sequential(\n",
    "            nn.Linear(1920,512),\n",
    "            torch.nn.LeakyReLU(),\n",
    "        )\n",
    "#     @torchsnooper.snoop()\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        The forward operation of each training step of the neural network model\n",
    "        '''\n",
    "        batch_size = x.size(0)        # Read the input batch size\n",
    "        x = x.unsqueeze(1)            # x.size = (Batch Size, 28, 28) -> (Batch Size, 1, 28, 28), the inserted dimension corresponds to channel dimension (channel = 1)\n",
    "        x = self.feature_extrator(x)  # Feature extraction\n",
    "        x = x.view(batch_size, -1)    # Flatten the output feature map into a 1D feature vector\n",
    "        x = self.fc_classifier(x)     # The size of output x should have a shape of [BATCH_SIZE, 10], where the second dimension corresponds to the probability vector of 10 hand written digits\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c63d95-71e8-4e38-be7b-f8485ec32e98",
   "metadata": {},
   "source": [
    "After building the CNN module, it's very likely that it won't work, because you haven't adjust the parameters, especially the number of channels in the first `nn.Linear` layer of `self.fc_classifier`. The input of this layer has to be the length of flattened feature map. Otherwise, PyTorch will spit out a size mismatch error.\n",
    "\n",
    "The size of feature vector can be calculated by hand, what I usually do is to run a quick forward pass with `torchsnooper.snoop()` to look at the size of tensor. that will naturally tell you the size of the feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb31086-df70-4652-b56e-59f08afbd34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull out 1 event from the dataset\n",
    "test_event, test_label = next(iter(MNISTDataset(plot=False)))\n",
    "test_event = test_event.unsqueeze(0).float() # (1,28,28) -> (1,1,28,28), inserting 1 more dimensions as batch dim\n",
    "test_network = CNN()\n",
    "print(test_network(test_event))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b7ebbf-6f17-4a6b-9b52-7d973552b3d7",
   "metadata": {},
   "source": [
    "In this test, you'd like to check for a few things:\n",
    "- There is no error spit out when running this function\n",
    "- The `print(test_network(test_event))` line will print out a tensor, as expected\n",
    "- The output tensor should have a size of (1,10), where 1 correspnd to the batch size (here we feed in only 1 event, so the batch size is 1), the second dimension correspond to the network output for each digit (0-9, 10 digits in total).\n",
    "\n",
    "After all criteria are satisfied, you will be able to proceed to the next step. $\\textbf{Important: Make sure you comment out the `torchsnooper.snoop()` line in front of forward() and rerun the CNN block before you proceed to the training, otherwise you will get a million lines of output from torchsnooper.}$\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0668d36f-fb7b-43fc-8090-59d00c9d0f1e",
   "metadata": {},
   "source": [
    "## Part III: Training\n",
    "Again, I'd like to re-emphasize that 𝐌𝐚𝐤𝐞 𝐬𝐮𝐫𝐞 𝐲𝐨𝐮 𝐜𝐨𝐦𝐦𝐞𝐧𝐭 𝐨𝐮𝐭 𝐭𝐡𝐞 𝐭𝐨𝐫𝐜𝐡𝐬𝐧𝐨𝐨𝐩𝐞𝐫.𝐬𝐧𝐨𝐨𝐩() 𝐥𝐢𝐧𝐞 𝐢𝐧 𝐟𝐫𝐨𝐧𝐭 𝐨𝐟 𝐟𝐨𝐫𝐰𝐚𝐫𝐝() 𝐚𝐧𝐝 𝐫𝐞𝐫𝐮𝐧 𝐭𝐡𝐞 𝐂𝐍𝐍 𝐛𝐥𝐨𝐜𝐤 before you dive into this section.\n",
    "\n",
    "The training of neural network can be done in a simple for-loop. First, let's define a few parameters:\n",
    "\n",
    "- `NUM_EPOCHS` : One epoch means training through the data set once, so number of epochs means the # of times we loop through the entire dataset.\n",
    "- `LEARNING_RATE`: As we discussed in lecture I, learning rate is the step with of Gradient Descent algorithm.\n",
    "- `DEVICE`: The device we'd like to use to train the network. Our model and data need to be fed into the device.\n",
    "- `BATCH_SIZE`: Size of minibatch in SGD mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c02dd5-f647-4a30-b362-fbf86475186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # This says if GPU is available, use GPU, otherwise use CPU\n",
    "NUM_EPOCHS =3\n",
    "LEARNING_RATE =1e-2 # 1e-2 is a good learning rate for general purpose\n",
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca64bb8b-fd89-4bc2-8ca9-9bade3ec829f",
   "metadata": {},
   "source": [
    "Next, we need to define several important instances for training purpose:\n",
    "- `classifier`: The neural network model we defined and tested earlier\n",
    "- `criterion`: The loss function we'd like to train against. Since we'd like to classify each image into class of 0-9 digits, we use `CrossEntropyLoss` as our loss function(https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss)\n",
    "- `Optimizer`: The optimizer of neural network model. Here we will use the SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0208baa5-e1bc-44cf-bd01-6db73ec3e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_classifier():\n",
    "    classifier = CNN() # Define CNN neural network classifier\n",
    "    classifier.to(DEVICE)     # Send the classifier to DEVICE as we defined earlier\n",
    "\n",
    "    print(\"# of params in model: \", sum(x.numel() for x in classifier.parameters()))\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(DEVICE)\n",
    "\n",
    "    #Define the optimizer\n",
    "    optimizer = torch.optim.SGD(classifier.parameters(),lr=LEARNING_RATE)\n",
    "    \n",
    "    return classifier, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4505e713-f114-44d5-bb86-c0b076828db5",
   "metadata": {},
   "source": [
    "The last thing before training is to define a data loader. Data loader takes in a `Dataset()` object, and convert it to a iterator for training purpose. We will split our dataset into `train_loader` and `test_loader`, and only use `train_loader` to train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50846bc-88e3-4f45-a4da-fc312a0677a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader():\n",
    "    dataset = MNISTDataset(plot=False)\n",
    "    #Get the indices of train dataset and test dataset correspondingly, indices [0:train_test_split] is the training dataset, indices [train_test_split, len(dataset)] is the test dataset.\n",
    "    train_test_split = dataset.get_train_test_split()\n",
    "    train_indices, val_indices = list(range(train_test_split)), list(range(train_test_split,len(dataset)))\n",
    "\n",
    "    #Shuffle the two indices list\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(val_indices)\n",
    "\n",
    "    # Define two subset random sampler to sample events according to the training indices\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    # Finally, define the loader by passing in the dataset, batch size and corresponding sampler\n",
    "    # Note that the number of data in each sub-dataset might not be divisibe by the batch size, so drop_last=True drops the last batch with all the residual events.\n",
    "    train_loader = data_utils.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler, drop_last=True)\n",
    "    test_loader = data_utils.DataLoader(dataset, batch_size=BATCH_SIZE,sampler=valid_sampler,  drop_last=True)\n",
    "    \n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c6a93a-74e4-4573-af8b-3faf2f874899",
   "metadata": {},
   "source": [
    "Having all the tools at hand, we can proceed to train our CNN. The training of CNN is done in the following steps:\n",
    "- Loop through all events in the train_loader\n",
    "- For each step, convey both image and label to the DEVICE\n",
    "- Feed image into classifier to produce network outpu\n",
    "- Feed network output and label into the loss function(criterion), to calculate the loss value\n",
    "- Back-propagate the loss to update gradient\n",
    "- Ask the optimizer to optimize for 1 step, then clear the gradient in the network\n",
    "After each epoch of training, we will also store the training loss and classification accuracy for the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ec5f5-b236-4cde-8fea-36f8e6543055",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier, criterion, optimizer = set_up_classifier()\n",
    "train_loader, test_loader = get_dataloader()\n",
    "\n",
    "loss_values = []\n",
    "accuracy_values = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (images, labels) in tqdm(enumerate(train_loader)):\n",
    "        classifier.train() # This line set the neural network to train mode, some layers perform differently in train and test mode.\n",
    "        \n",
    "        images = images.to(DEVICE).float()\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        #Train the CNN classifier\n",
    "        outputs  = classifier(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Back-propagate loss to update gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform gradient descent to update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # reset gradient to 0 on all parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print('\\rEpoch [{0}/{1}], Iter [{2}/{3}] Loss: {4:.4f}'.format(\n",
    "        epoch+1, NUM_EPOCHS, i+1, len(train_loader),\n",
    "        loss.item(), end=\"\"),end=\"\")\n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    #After every epoch, evaluate the validation accuracy on the test loader\n",
    "    num_accurate = 0\n",
    "    num_images = 0\n",
    "    for images,labels in tqdm(test_loader):\n",
    "\n",
    "        classifier.eval() # This line set the neural network to evaluation mode, some layers perform differently in train and test mode.\n",
    "        \n",
    "        #While validating the network, we do not want it to produce any gradient. This will also save us time/memory\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Convey images to device, then feed it to the neural network for network output\n",
    "            images = images.to(DEVICE).float()\n",
    "            outputs  = classifier(images)\n",
    "            \n",
    "            # Get classification decision by reading out the maximum value on the 10-dimensional vector\n",
    "            decision = torch.argmax(outputs, dim=-1)\n",
    "            decision = decision.cpu().data.numpy().flatten() # copy decision to CPU and convert it to a numpy array\n",
    "            labels = labels.cpu().data.numpy().flatten()\n",
    "            \n",
    "            # Update the list of truth value and network predictions in last epoch:\n",
    "            if epoch == (NUM_EPOCHS-1):\n",
    "                y_true += list(labels)\n",
    "                y_pred += list(decision)\n",
    "            \n",
    "            #Calculate accuracy by # of correct prediction / total numbers\n",
    "\n",
    "            num_accurate += np.sum((decision - labels) == 0)\n",
    "            num_images += len(decision)\n",
    "    accuracy_values.append(num_accurate/num_images)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e5c9d1-db89-4c2b-b96a-3a4b636089bf",
   "metadata": {},
   "source": [
    "## Part IV: Evaluate Training Results\n",
    "After training, we will be able to evaluate our training results.\n",
    "\n",
    "First, let's plot the learning curve, that is, the loss value with respect to the epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de826c4-8dba-46d3-acef-434c2ea78c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(NUM_EPOCHS).astype(int), loss_values)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cross Entropy Loss [a.u.]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f08339-c11f-4051-b802-f6312766779f",
   "metadata": {},
   "source": [
    "You should find that the loss drops as you train the network with more and more epochs. If not, what could be the possible cause of it?\n",
    "\n",
    "Next, let's plot the accuracy curve. That is, the accuracy with respect to epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c21b2-0845-4fc8-8057-fc2967d63123",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(NUM_EPOCHS).astype(int), np.array(accuracy_values)*100.0)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Classification Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055b3f6c-c042-4cb6-bac1-d46b24327e8f",
   "metadata": {},
   "source": [
    "Lastly, for a multi-class classification problem, it's useful to plot the confusion matrix. We've collected `y_true` and `y_pred` during the training, thus we can directly plot it out using `sklearn.metrics.confusion_matrix`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9bf1ec-cdd5-453b-ae37-5cd210e7e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.imshow(cm,norm=matplotlib.colors.LogNorm(), cmap = \"Reds\") #Making the z-direction logarithmic scale\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Ground Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aa2535-6e5a-4290-bdfa-8b35aa18ea47",
   "metadata": {},
   "source": [
    "$\\textbf{Questions}$: How to understand the confusion matrix?\n",
    "\n",
    "Finally, let's pull out a image, feed it into the trained classifier, and look at the network output. We use a Softmax function to wrap the network output to convert it to probability distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a3f8b-d582-443d-bfa7-441ac7a1b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull out 1 event from the dataset\n",
    "test_event, test_label = MNISTDataset(plot=False)[np.random.randint(len(test_dset))]\n",
    "plt.imshow(test_event)\n",
    "test_event = test_event.unsqueeze(0).float() # (1,28,28) -> (1,1,28,28), inserting 1 more dimensions as batch dim\n",
    "decision = torch.softmax(classifier(test_event),dim=-1).cpu().data.numpy().flatten()\n",
    "for i in range(len(decision)):\n",
    "    print(\"The probability of this image being digit %d is %.1f%%\"%(i, decision[i]*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b59cf16-b8a6-4b8e-9254-c51bcc0c755a",
   "metadata": {},
   "source": [
    "You can run the previous block multiple times to extract classification decisions on different images.\n",
    "\n",
    "$\\textbf{Questions}$: Are these image correctly classified? What kind of hand-written digits is classified with high confidence, and what kind of hand-written digits confuse the machine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0303dd75-d36e-4d51-99af-d099a9c6b7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-1.7.1",
   "language": "python",
   "name": "pytorch-1.7.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
