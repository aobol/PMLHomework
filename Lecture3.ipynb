{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c6fb08-3cea-42b3-a2e8-827f4ed02f70",
   "metadata": {},
   "source": [
    "# Lecture III: Recurrent Neural Network\n",
    "This homework aims to help you understand Recurrent Neural Network by building a RNN classifier on Japanese Vowels from different male speakers. \n",
    "\n",
    "As usual, we will first import some useful modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0530854e-4815-4698-b3a7-aaefc100077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=E1101,R,C\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import gzip\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,RobustScaler,MinMaxScaler\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torchsnooper\n",
    "from torch.cuda.amp import autocast \n",
    "torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e98b4-b7e4-4bbd-9071-20ef83c290b5",
   "metadata": {},
   "source": [
    "The Japanese Vowel dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Japanese+Vowels). For a detailed description, please look at this website. First, we will download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794b683-14a2-424a-9a0d-a95cc0184eb8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget -nc https://archive.ics.uci.edu/ml/machine-learning-databases/JapaneseVowels-mld/ae.train\n",
    "!wget -nc https://archive.ics.uci.edu/ml/machine-learning-databases/JapaneseVowels-mld/ae.test\n",
    "!wget -nc https://archive.ics.uci.edu/ml/machine-learning-databases/JapaneseVowels-mld/size_ae.train\n",
    "!wget -nc https://archive.ics.uci.edu/ml/machine-learning-databases/JapaneseVowels-mld/size_ae.test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4672577b-f628-42f9-8594-6ac453311a6f",
   "metadata": {},
   "source": [
    "This dataset contains 9 male japanese speaker pronouncing the utterance /ae/. The data is decoded by Linear Predictive Coding. A detail of LPC can be found [here](https://en.wikipedia.org/wiki/Linear_predictive_coding). Each utterance contains 12 LPC basis, thus for each time index, the time series will contain 12 channels.\n",
    "\n",
    "## Part I: Dataset\n",
    "\n",
    "First, we will need to prepare the dataset using `Dataset()` class. Since in last home work, I have gone through the detail of this class, I will skip most of the technical details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed9593-fdff-4f46-b767-686586320380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JapaneseVowelDataset(Dataset):\n",
    "\n",
    "    def __init__(self,plot=True):\n",
    "        self.max_length = 29 # The maximum possible length of each utterance contains 29 samples\n",
    "        self.num_LPC = 12    # The LPC spectrum contains 12 coefficients, so the data shape will be [29,12]\n",
    "        \n",
    "        train_data, train_label = self.read_vowels(\"ae.train\",\"size_ae.train\")\n",
    "        test_data, test_label = self.read_vowels(\"ae.test\",\"size_ae.test\")\n",
    "        \n",
    "        self.size = len(train_data) + len(test_data)\n",
    "        self.train_test_split = len(train_data)\n",
    "        \n",
    "        self.data = train_data + test_data\n",
    "        self.labels = train_label + test_label\n",
    "        \n",
    "        if plot:\n",
    "            self.plot_data()\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        This function returns the size of overall dataset\n",
    "        '''\n",
    "        return self.size\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        This function extract a single entry from the dataset at the given index idx\n",
    "        In this dataset, the data has variable length, so we need to pad \n",
    "        the LPC coefficients to have the same length for training purpose\n",
    "        '''\n",
    "        output = np.zeros((self.max_length, self.num_LPC))\n",
    "        data = self.data[idx]\n",
    "        output[:data.shape[0]] += data\n",
    "        return output, self.labels[idx]\n",
    "    \n",
    "    def get_train_test_split(self):\n",
    "        '''\n",
    "        This function get the train test split size of the dataset\n",
    "        '''\n",
    "        return self.train_test_split\n",
    "    \n",
    "    def read_vowels(self,file, size_file):\n",
    "        vowel_units = []\n",
    "        speaker_size = []\n",
    "        labels = []\n",
    "        #Read out the LPC value of all vowels\n",
    "        with open(file, \"r\") as f:\n",
    "            current_vowel = []\n",
    "            for line in f.readlines():\n",
    "                if line == '\\n':\n",
    "                    vowel_units.append(np.array(current_vowel))\n",
    "                    current_vowel = []\n",
    "                    continue\n",
    "                current_vowel.append(np.array(line.strip().split(\" \"),dtype=float).tolist())\n",
    "        #Read out the size of samples by 9 speakers\n",
    "        with open(size_file, \"r\") as f:\n",
    "            speaker_size = np.array(f.readline().strip().split(\" \"),dtype=int)\n",
    "            assert len(speaker_size) == 9 # If speaker size is not 9, then there's something wrong\n",
    "        #Assign a label to each speaker, speaker 1 == 0 .....speaker 9 == 8:\n",
    "        for speaker_label in range(9):\n",
    "            labels += [speaker_label] * speaker_size[speaker_label]\n",
    "        # Check if the number of label equals to number of data\n",
    "        # If not, there is something wrong\n",
    "        assert len(vowel_units) == len(labels)\n",
    "        return vowel_units, labels\n",
    "            \n",
    "                    \n",
    "        \n",
    "    \n",
    "    def plot_data(self):\n",
    "        '''\n",
    "        This function plots the LPC spectrum of 9 random utterances\n",
    "        '''\n",
    "        plt.figure(figsize=(20,12))\n",
    "        sample_index = np.random.randint(low=0,high=self.__len__(), size = 9)\n",
    "        for i in range(9):\n",
    "            plt.subplot(3,3, i+1)\n",
    "            voice, label = self.__getitem__(sample_index[i])\n",
    "            utt_length = voice.shape[0]\n",
    "            for i in range(voice.shape[-1]):\n",
    "                plt.plot(np.arange(utt_length), voice[:,i])\n",
    "            plt.xlabel(\"Time Index\")\n",
    "            plt.ylabel(\"LPC Coefficients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c974e04-b014-4e97-bda1-caee06d0b0bb",
   "metadata": {},
   "source": [
    "Similarly, we can check the form of data by plotting the LPC spectrum coefficients. The trailing 0s comes from the padding we performed within the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279aa8a-b9d5-4cfc-82e0-1cfbbe7c6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "JapaneseVowelDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d62b0d-ccc5-4a43-8b3c-19d6f66c7a6d",
   "metadata": {},
   "source": [
    "## Part II: Recurrent Neural Network\n",
    "In this part, we will buid an LSTM based recurrent neural network model to analyze the input LPC spectrum. Since this is the second time we build a NN, I will leave most of the work to you. Some useful resources include:\n",
    "- [nn.LSTM](https://pytorch.org/docs/master/generated/torch.nn.LSTM.html#lstm)\n",
    "- [nn.Linear](https://pytorch.org/docs/master/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear)\n",
    "- [nn.LeakyReLU](https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html?highlight=leakyrelu#torch.nn.LeakyReLU)\n",
    "- [nn.Sequential](https://pytorch.org/docs/master/generated/torch.nn.Sequential.html?highlight=sequential#torch.nn.Sequential)\n",
    "\n",
    "Some useful tips for building the recurrent neural network:\n",
    "- Read [nn.LSTM](https://pytorch.org/docs/master/generated/torch.nn.LSTM.html#lstm) carefully, make sure you understand the input and output shape of the LSTM layer. The [nn.LSTM](https://pytorch.org/docs/master/generated/torch.nn.LSTM.html#lstm) layer outputs `output, (h_n, c_n)`:\n",
    "    -  `output` has a shape of `(Seq_len,Batch_size,num_direction*hidden_size)`, it contains all intermediate hidden state outputs of the last layer. Note that there is a `batch_first` flag in LSTM input which may change the output shape to `(Batch_size,Seq_len,num_direction*hidden_size)`. If you want to select only the last hidden state, you shoud simply index the `Seq_len` dimension.\n",
    "    -  `(h_n, c_n)` contains the last hidden state outputs of each layers\n",
    "- Remember that you want to select only the the **last hidden state output**. You can either extract it from `output` or `h_n`.\n",
    "- Then you will feed the **last hidden state output** into `self.fc_classifier`. You will need to adjust the `hidden_size` of LSTM output and the input size of first fully connected layer in `self.fc_classifier` to match\n",
    "- Make sure the output of RNN model has the shape of [BATCHSIZE,9], since we have 9 speakers in the dataset we'd like to classify into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daaf391-3ef6-46ae-aafa-ddc30274416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        '''\n",
    "        Initialize the RNN. As mentioned in the lecture, RNN contains 2 parts:\n",
    "            A feature extractor based on LSTM network\n",
    "            A fully connected classifier\n",
    "        '''\n",
    "#     @torchsnooper.snoop()\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        The forward operation of each training step of the neural network model\n",
    "        '''\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13ef562-89de-4973-a968-dd564a6d2d6e",
   "metadata": {},
   "source": [
    "Similar to what we did in Lecture 2 homework, we will pull out 1 event from the dataset, and use `torchsnooper.snoop()` to check the network structure. Before proceeding to the next part, you may want to stare at the tensor output of `torchsnooper.snoop()` carefully to understand how the data is flowing within the RNN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f813b8e-9e76-4385-a50a-daab83da3eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull out 1 event from the dataset\n",
    "test_event, test_label = next(iter(JapaneseVowelDataset(plot=False)))\n",
    "test_event = torch.FloatTensor(test_event).unsqueeze(0) # Insert batch dimension\n",
    "test_network = RNN()\n",
    "print(test_network(test_event,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f1b65e-930e-4659-85fe-d494c36213fb",
   "metadata": {},
   "source": [
    "## Part III: Training\n",
    "After building the neural network, we train it the same way as we did in Lecture 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce807da-2b10-49fc-9c83-78e88118d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # This says if GPU is available, use GPU, otherwise use CPU\n",
    "NUM_EPOCHS =20\n",
    "LEARNING_RATE =1e-3 # 1e-2 is a good learning rate for general purpose\n",
    "BATCH_SIZE=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5bbdcd-e09c-4e02-bd04-b9214d5f9e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_classifier():\n",
    "    classifier = RNN() # Define CNN neural network classifier\n",
    "    classifier.to(DEVICE)     # Send the classifier to DEVICE as we defined earlier\n",
    "\n",
    "    print(\"# of params in model: \", sum(x.numel() for x in classifier.parameters()))\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(DEVICE)\n",
    "\n",
    "    #Define the optimizer\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(),lr=LEARNING_RATE)\n",
    "    \n",
    "    return classifier, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da4c371-4de8-4a45-9ce3-03735bdfdab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader():\n",
    "    dataset = JapaneseVowelDataset(plot=False)\n",
    "    #Get the indices of train dataset and test dataset correspondingly, indices [0:train_test_split] is the training dataset, indices [train_test_split, len(dataset)] is the test dataset.\n",
    "    train_test_split = dataset.get_train_test_split()\n",
    "    train_indices, val_indices = list(range(train_test_split)), list(range(train_test_split,len(dataset)))\n",
    "\n",
    "    #Shuffle the two indices list\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(val_indices)\n",
    "\n",
    "    # Define two subset random sampler to sample events according to the training indices\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    # Finally, define the loader by passing in the dataset, batch size and corresponding sampler\n",
    "    # Note that the number of data in each sub-dataset might not be divisibe by the batch size, so drop_last=True drops the last batch with all the residual events.\n",
    "    train_loader = data_utils.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler, drop_last=True)\n",
    "    test_loader = data_utils.DataLoader(dataset, batch_size=BATCH_SIZE,sampler=valid_sampler,  drop_last=True)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146fbea3-f699-491d-b524-2a5c3925f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier, criterion, optimizer = set_up_classifier()\n",
    "train_loader, test_loader = get_dataloader()\n",
    "\n",
    "loss_values = []\n",
    "accuracy_values = []\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (utterances, labels) in tqdm(enumerate(train_loader)):\n",
    "        classifier.train() # This line set the neural network to train mode, some layers perform differently in train and test mode.\n",
    "        \n",
    "        utterances = utterances.to(DEVICE).float()\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        #Train the RNN classifier\n",
    "        outputs  = classifier(utterances)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Back-propagate loss to update gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform gradient descent to update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # reset gradient to 0 on all parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print('\\rEpoch [{0}/{1}], Iter [{2}/{3}] Loss: {4:.4f}'.format(\n",
    "        epoch+1, NUM_EPOCHS, i+1, len(train_loader),\n",
    "        loss.item(), end=\"\"),end=\"\")\n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    #After every epoch, evaluate the validation accuracy on the test loader\n",
    "    num_accurate = 0\n",
    "    num_images = 0\n",
    "    for utterances,labels in tqdm(test_loader):\n",
    "\n",
    "        classifier.eval() # This line set the neural network to evaluation mode, some layers perform differently in train and test mode.\n",
    "        \n",
    "        #While validating the network, we do not want it to produce any gradient. This will also save us time/memory\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Convey images to device, then feed it to the neural network for network output\n",
    "            utterances = utterances.to(DEVICE).float()\n",
    "            outputs  = classifier(utterances)\n",
    "            \n",
    "            # Get classification decision by reading out the maximum value on the 10-dimensional vector\n",
    "            decision = torch.argmax(outputs, dim=-1)\n",
    "            decision = decision.cpu().data.numpy().flatten() # copy decision to CPU and convert it to a numpy array\n",
    "            labels = labels.cpu().data.numpy().flatten()\n",
    "            \n",
    "            # Update the list of truth value and network predictions in last epoch:\n",
    "            if epoch == (NUM_EPOCHS-1):\n",
    "                y_true += list(labels)\n",
    "                y_pred += list(decision)\n",
    "            \n",
    "            #Calculate accuracy by # of correct prediction / total numbers\n",
    "\n",
    "            num_accurate += np.sum((decision - labels) == 0)\n",
    "            num_images += len(decision)\n",
    "    accuracy_values.append(num_accurate/num_images)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4e5b65-aaa5-4e38-904f-6eab58cae9d6",
   "metadata": {},
   "source": [
    "Does the training loss reduce to a value below 2.0? If not, there are a few things you can try:\n",
    "- Playing with the learning rate and see how it affects the training.\n",
    "- Adding more layers by changing the `num_layers` variable in LSTM class.\n",
    "- In RNN, it's very easy to get gradient vanishing in the `self.classifier` part, try to use **only one fully connected layers**.\n",
    "- Set `bidirectional=True` for the LSTM layer. **Caveats: Doing this will change the shape of LSTM output*\n",
    "\n",
    "If you can get the loss to reduce to well below 2.0, proceed to the last part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188f66c6-7953-4612-9669-4d1c2d6fa67f",
   "metadata": {},
   "source": [
    "## Part IV: Evaluate Training Results\n",
    "After training, we will be able to evaluate our training results.\n",
    "\n",
    "First, let's plot the learning curve, that is, the loss value with respect to the epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb95cf-8cd4-442c-b750-49bfc6dabef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(NUM_EPOCHS).astype(int), loss_values)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cross Entropy Loss [a.u.]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c5967f-ca80-4b61-aa36-807484a3318e",
   "metadata": {},
   "source": [
    "You should find that the loss drops as you train the network with more and more epochs. If not, what could be the possible cause of it?\n",
    "\n",
    "Next, let's plot the accuracy curve. That is, the accuracy with respect to epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b3340-0c8e-4f19-b170-53d90d83358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(NUM_EPOCHS).astype(int), np.array(accuracy_values)*100.0)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Classification Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b8ab7-cee9-47a9-83b2-1e857fe66810",
   "metadata": {},
   "source": [
    "Lastly, let's plot the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2f101-0b61-493e-b6b8-a8b6e5853fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.imshow(cm,norm=matplotlib.colors.LogNorm(), cmap = \"Reds\") #Making the z-direction logarithmic scale\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Ground Truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c31908-605f-4837-beb1-47e03870f236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-1.7.1",
   "language": "python",
   "name": "pytorch-1.7.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
